{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f969315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import dlib\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict, deque\n",
    "from transformers import ViTForImageClassification\n",
    "import torchvision.transforms as transforms\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea61ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "datFile = r\"E:\\UET\\HMI\\Pj\\shape_predictor_68_face_landmarks.dat\"\n",
    "face_rec_model_path = r\"E:\\UET\\HMI\\Pj\\dlib_face_recognition_resnet_model_v1.dat\"\n",
    "video_path = r\"E:\\UET\\HMI\\Pj\\Trich doan 3 (ly truong - me mo).mp4\"\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor(datFile)\n",
    "face_encoder = dlib.face_recognition_model_v1(face_rec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f80de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = ['Genuine Disgust', 'Posed Disgust', 'Genuine Happiness', 'Posed Happiness',\n",
    "            'Genuine Fear', 'Posed Fear', 'Genuine Anger', 'Posed Anger',\n",
    "            'Genuine Surprise', 'Posed Surprise', 'Genuine Sadness', 'Posed Sadness']\n",
    "num_classes = len(emotion_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d28270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model.classifier = torch.nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(r\"E:\\UET\\HMI\\Pj\\vit_model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514f0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 12\n",
    "IMG_SIZE = 224\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e56b575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUNCTIONS ===\n",
    "def extract_faces(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces, scores, _ = face_detector.run(gray, 1, 0)\n",
    "    results = []\n",
    "    for face in faces:\n",
    "        if face.width() < 20 or face.height() < 20:\n",
    "            continue\n",
    "        try:\n",
    "            landmarks = landmark_predictor(gray, face)\n",
    "            face_chip = dlib.get_face_chip(image, landmarks)\n",
    "            embedding = np.array(face_encoder.compute_face_descriptor(face_chip))\n",
    "            x_min, y_min, x_max, y_max = face.left(), face.top(), face.right(), face.bottom()\n",
    "            face_crop = image[y_min:y_max, x_min:x_max]\n",
    "            resized = cv2.resize(face_crop, (224, 224))\n",
    "            results.append((resized, (x_min, y_min, x_max, y_max), embedding))\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def calculate_optical_flow(prev_frame, next_frame):\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None,\n",
    "                                        0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    return flow\n",
    "\n",
    "def calculate_motion_intensity(flow):\n",
    "    mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    return np.sum(mag)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PROCESSING ===\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_id = 0\n",
    "person_id_counter = 0\n",
    "\n",
    "active_tracks = {}\n",
    "all_segments = []\n",
    "\n",
    "# Helper functions\n",
    "def compute_iou(box1, box2):\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    boxBArea = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    return interArea / float(boxAArea + boxBArea - interArea + 1e-5)\n",
    "\n",
    "def center_distance(box1, box2):\n",
    "    cx1 = (box1[0] + box1[2]) / 2\n",
    "    cy1 = (box1[1] + box1[3]) / 2\n",
    "    cx2 = (box2[0] + box2[2]) / 2\n",
    "    cy2 = (box2[1] + box2[3]) / 2\n",
    "    return np.sqrt((cx1 - cx2)**2 + (cy1 - cy2)**2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_id += 1\n",
    "    faces = extract_faces(frame)\n",
    "    matched_ids = set()\n",
    "\n",
    "    for face_img, box, embedding in faces:\n",
    "        matched = False\n",
    "        for pid in list(active_tracks):\n",
    "            data = active_tracks[pid]\n",
    "            avg_embedding = np.mean(data['embeddings'], axis=0) if data['embeddings'] else data['embedding']\n",
    "            sim = cosine_similarity(embedding, avg_embedding)\n",
    "            iou = compute_iou(box, data['last_box'])\n",
    "            dist = center_distance(box, data['last_box'])\n",
    "\n",
    "            if sim > 0.3 or iou > 0.2 or dist < 60:\n",
    "                flow = calculate_optical_flow(data['last_face'], face_img) if data['last_face'] is not None else None\n",
    "                intensity = calculate_motion_intensity(flow) if flow is not None else 0\n",
    "                data['flow_buffer'].append((frame_id, intensity))\n",
    "\n",
    "                buffer = data['flow_buffer']\n",
    "                if not data['active']:\n",
    "                    if len(buffer) >= 3 and all(buffer[i][1] < buffer[i+1][1] for i in range(-3, -1)):\n",
    "                        data['active'] = True\n",
    "                        data['segment'] = [(frame_id, face_img, box)]\n",
    "                else:\n",
    "                    data['segment'].append((frame_id, face_img, box))\n",
    "                    if len(buffer) >= 4 and all(buffer[i][1] > buffer[i+1][1] for i in range(-4, -1)):\n",
    "                        all_segments.append((pid, data['segment']))\n",
    "                        data['segment'] = []\n",
    "                        data['flow_buffer'].clear()\n",
    "                        data['active'] = False\n",
    "\n",
    "                data['last_face'] = face_img\n",
    "                data['last_box'] = box\n",
    "                data['embedding'] = embedding\n",
    "                data['embeddings'].append(embedding)\n",
    "                data['last_seen'] = frame_id\n",
    "                matched_ids.add(pid)\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            active_tracks[person_id_counter] = {\n",
    "                'last_seen': frame_id,\n",
    "                'last_box': box,\n",
    "                'last_face': face_img,\n",
    "                'embedding': embedding,\n",
    "                'embeddings': deque([embedding], maxlen=5),\n",
    "                'flow_buffer': deque(maxlen=10),\n",
    "                'segment': [],\n",
    "                'active': False\n",
    "            }\n",
    "            matched_ids.add(person_id_counter)\n",
    "            person_id_counter += 1\n",
    "\n",
    "    # Clean up stale tracks\n",
    "    for pid in list(active_tracks):\n",
    "        if frame_id - active_tracks[pid]['last_seen'] > 50:\n",
    "            del active_tracks[pid]\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9c12cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dán nhãn và render\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "out = cv2.VideoWriter(\"output_emotion_1.mp4\",\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                      fps, (width, height))\n",
    "\n",
    "frame_map = defaultdict(list)\n",
    "\n",
    "for pid, segment in all_segments:\n",
    "    if len(segment) < 2:\n",
    "        continue\n",
    "    onset_face = segment[0][1]\n",
    "    highest = -1\n",
    "    apex_idx = 0\n",
    "    for idx, (_, face_img, _) in enumerate(segment):\n",
    "        try:\n",
    "            flow = calculate_optical_flow(onset_face, face_img)\n",
    "            intensity = calculate_motion_intensity(flow)\n",
    "            if intensity > highest:\n",
    "                highest = intensity\n",
    "                apex_idx = idx\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    apex_face = segment[apex_idx][1]\n",
    "    input_tensor = transform(apex_face).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred_idx = torch.argmax(output.logits, dim=1).item()\n",
    "        emotion = emotion_labels[pred_idx]\n",
    "\n",
    "    for frame_id, _, box in segment:\n",
    "        frame_map[frame_id].append((box, emotion))\n",
    "\n",
    "current_frame_id = 1\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if current_frame_id in frame_map:\n",
    "        for box, emotion in frame_map[current_frame_id]:\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, emotion, (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "            cv2.putText(frame, f\"Person ID: {current_frame_id}\",\n",
    "                        (x1, y1 - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "    out.write(frame)\n",
    "    current_frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1315dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hoàn tất: final_output_with_audio.mp4\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Hoàn tất: final_output_with_audio.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
