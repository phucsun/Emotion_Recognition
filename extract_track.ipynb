{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56080e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTForImageClassification\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6345f972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Admin/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Admin/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Admin/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Admin/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\Admin/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_wts_path))\n"
     ]
    }
   ],
   "source": [
    "app = FaceAnalysis(name=\"buffalo_l\")  # có detector, landmark, arcface\n",
    "app.prepare(ctx_id=0)\n",
    "tracker = DeepSort(max_age=10000000000000000000000000000, n_init=3, nn_budget=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe86e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r\"E:\\UET\\HMI\\Pj\\Input\\Trich doan 3 (ly truong - me mo).mp4\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b299dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = ['Genuine Disgust', 'Posed Disgust', 'Genuine Happiness', 'Posed Happiness',\n",
    "            'Genuine Fear', 'Posed Fear', 'Genuine Anger', 'Posed Anger',\n",
    "            'Genuine Surprise', 'Posed Surprise', 'Genuine Sadness', 'Posed Sadness']\n",
    "num_classes = len(emotion_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575c8d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22456\\394440479.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"E:\\UET\\HMI\\Pj\\models\\vit_model.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model.classifier = torch.nn.Linear(in_features=768, out_features=num_classes, bias=True)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(r\"E:\\UET\\HMI\\Pj\\models\\vit_model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca21fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 12\n",
    "IMG_SIZE = 224\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c97cfb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2133b217",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Bỏ mặt nhỏ\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     detections\u001b[38;5;241m.\u001b[39mappend(([x1, y1, x2 \u001b[38;5;241m-\u001b[39m x1, y2 \u001b[38;5;241m-\u001b[39m y1], conf, face\u001b[38;5;241m.\u001b[39membedding))\n\u001b[1;32m---> 20\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m tracks:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m track\u001b[38;5;241m.\u001b[39mis_confirmed():\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\deep_sort_realtime\\deepsort_tracker.py:228\u001b[0m, in \u001b[0;36mDeepSort.update_tracks\u001b[1;34m(self, raw_detections, embeds, frame, today, others, instance_masks)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Update tracker.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mpredict()\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoday\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoday\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mtracks\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\deep_sort_realtime\\deep_sort\\tracker.py:97\u001b[0m, in \u001b[0;36mTracker.update\u001b[1;34m(self, detections, today)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Run matching cascade.\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m matches, unmatched_tracks, unmatched_detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Update track set.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track_idx, detection_idx \u001b[38;5;129;01min\u001b[39;00m matches:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\deep_sort_realtime\\deep_sort\\tracker.py:151\u001b[0m, in \u001b[0;36mTracker._match\u001b[1;34m(self, detections)\u001b[0m\n\u001b[0;32m    142\u001b[0m unconfirmed_tracks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    143\u001b[0m     i \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_confirmed()\n\u001b[0;32m    144\u001b[0m ]\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Associate confirmed tracks using appearance features.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m (\n\u001b[0;32m    148\u001b[0m     matches_a,\n\u001b[0;32m    149\u001b[0m     unmatched_tracks_a,\n\u001b[0;32m    150\u001b[0m     unmatched_detections,\n\u001b[1;32m--> 151\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_assignment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatching_cascade\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgated_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatching_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_age\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfirmed_tracks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Associate remaining tracks together with unconfirmed tracks using IOU.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m iou_track_candidates \u001b[38;5;241m=\u001b[39m unconfirmed_tracks \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m    162\u001b[0m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m unmatched_tracks_a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracks[k]\u001b[38;5;241m.\u001b[39mtime_since_update \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    163\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\dlib_env\\lib\\site-packages\\deep_sort_realtime\\deep_sort\\linear_assignment.py:137\u001b[0m, in \u001b[0;36mmatching_cascade\u001b[1;34m(distance_metric, max_distance, cascade_depth, tracks, detections, track_indices, detection_indices)\u001b[0m\n\u001b[0;32m    135\u001b[0m unmatched_detections \u001b[38;5;241m=\u001b[39m detection_indices\n\u001b[0;32m    136\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cascade_depth):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unmatched_detections) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# No detections left\u001b[39;00m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_id = 0\n",
    "track_faces = {}\n",
    "ax = plt.gca()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_id += 1\n",
    "    faces = app.get(frame)\n",
    "    detections = []\n",
    "    for face in faces:\n",
    "        x1, y1, x2, y2 = face.bbox.astype(int)\n",
    "        conf = face.det_score\n",
    "        if (x2 - x1) < 70 or (y2 - y1) < 70:\n",
    "            continue  # Bỏ mặt nhỏ\n",
    "        detections.append(([x1, y1, x2 - x1, y2 - y1], conf, face.embedding))\n",
    "\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        l, t, r, b = track.to_ltrb()\n",
    "        l, t, r, b = int(l), int(t), int(r), int(b)\n",
    "        rect = plt.Rectangle((l, t), r - l, b - t, linewidth=2,\n",
    "                         edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Thêm track ID\n",
    "        ax.text(l, t - 5, f'ID: {track_id}', color='yellow', fontsize=10, weight='bold',\n",
    "                bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "        face_crop = frame[t:b, l:r]\n",
    "        if track_id not in track_faces:\n",
    "            track_faces[track_id] = []\n",
    "        track_faces[track_id].append((frame_id, face_crop))\n",
    "\n",
    "    # Hiển thị bằng matplotlib trong Jupyter\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(frame_rgb)\n",
    "    plt.title(f\"Frame: {frame_id} - Active Tracks: {len(tracks)}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# In thông tin\n",
    "print(f\"Tổng số người đã track: {len(track_faces)}\")\n",
    "for tid, seq in track_faces.items():\n",
    "    print(f\"Track ID {tid}: {len(seq)} frames\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
